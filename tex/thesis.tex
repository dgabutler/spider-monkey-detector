\documentclass[11pt]{article}

\usepackage{natbib}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{setspace}
% \usepackage{hyperref}


\begin{document}

\doublespacing

\section{Introduction}

Threats to biodiversity in highly biodiverse regions such as rainforests are increasing (Alroy 2017). Understanding the full effects of these requires frequent monitoring on large enough scales \citep{underwood2005large,porter2009new} and over a sufficiently long period of time \citep{porter2005wireless}; however, at present there is a lack of sufficient effective monitoring systems \citep{proencca2017global}. It imperative to develop cost-effective monitoring techniques with the potential to be implemented at large scales and over long time periods in crucially important highly biodiverse regions such as rainforests. Due to technological and theoretical advancements, one emerging approach is the combination of innovations in machine learning with passive acoustic monitoring.

Passive acoustic monitoring (hereafter PAM) is the process of collecting acoustic data in the field using sensors such as microphones, to then analyse at a later point. The acoustic data collected can be used to answer a number of questions relating to the ecology and distribution of species \citep{browning2017passive}, which can for example be used in the design (location and habitat type) of protected areas \citep{rayment2009use}. 

It holds promise as an efficient surveying tool in hyperbiodiverse regions for a number of reasons. Acoustic monitoring approaches can reduce or eliminate biases inherent in other survey methods, including detection bias (as initial data collection is independent of observer skill level \citep{klingbeil2015bird}), temporal bias (which has shown in point count studies to result in missed behaviours and underestimated population sizes \citep{bridges2000temporal}, and biases caused by human disturbance \citep{alldredge2007time}. Meeting the requirements of more efficient surveying techniques, the area under surveyance can be increased for a comparatively lessened increase in cost, which can allow ecological questions to be tested on large scales \citep{wrege2017acoustic}. The data can be contributed to global repositories of biodiversity information, increasing the potential for widescale monitoring and modelling \citep{honrado2016fostering}. Furthermore, a key benefit is that the data collected forms a permanent record: survey analyses are able to repeatable, different ecological questions can be investigated using the same data (e.g. \citep{newson2017potential}, and factors such as changes of community composition can be looked at \citep{rogers2013density}. PAM techniques may make it possible to monitor otherwise unobservable cryptic species and behaviours \citep{wrege2017acoustic}. Where suitable, another important application could be in more viably evaluating the effectiveness of conservation actions \citep{wrege2017acoustic}, a critical stage which is too often overlooked in conservation science \citep{ferraro2006money,legg2006most}.

Further reasons why PAM could be particularly beneficial in regions such as rainforests include that acoustic monitoring approaches are much less seasonally restricted \citep{shonfield2017autonomous} (important in tropical biomes which often have prohibitive seasonal weather), and that it enables surveying of areas where direct observation of species may not be feasible. Additionally, the general advantage of associated reduction in observer effort when using PAM approaches \citep{digby2013practical} are accentuated as survey sites in hyperbiodiverse areas are often remote and potentially difficult to access, allowing for data to be collected over longer time frames more easily. Recent significant reductions in cost of surveying devices (from hundreds of pounds to as little as £40 per unit for the recently developed AudioMoth devices \citep{hill2018audiomoth}, as well as improvements to their memory capacity and factors such as weatherproofing \citep{fanioudakis2017deep}, have also increased the potential and scale of PAM analyses. However, these improvements, while allowing for a lot of data to be collected very efficiently, can cause significant challenges when it comes to the use of the data.

Although there is broad potential in applying PAM approaches in highly biodiverse regions, managing and analysying the terrabytes of data that these investigations can collect has been a significant problem \citep{villanueva2012pumilio,shonfield2017autonomous}. Extraction of the sounds of interest requires an expert to spend a large amount of time listening to the recordings, and rarely quantified souces of bias can be introduced at this stage \citep{digby2013practical}. As a result of the processing time required, it is common that only a fraction of data collected is able to be used \citep{kobayasi2012classification}. There has therefore been a strong incentive to incorporate techniques from the field of machine learning (hereafter ML), in which algorithms can be designed that are capable of automating the processing element of the task. 

Despite there being a documented lack of communication between the two fields of research \citep{thessen2016adoption}, traditional ML techniques such as support vector machines (REF REF REF), random forest (REF REF REF), and naive bayes (REF REF REF) have been applied to bioacoustic datasets for wide a variety of taxa (see comprehensive recent review by \cite{knight2017recommendations}). These algorithms compare key hand-designed features of input audio data - temporal (e.g. duration) or spectral (e.g. peak frequency) - with those learned from a dataset of labelled training examples. However, while excellent results have been reported using these traditional methods (although classification algorithms coming even close to expert observer accuracy rates are not common \citep{ovaskainen2018animal}) few attempts have been made to combine automated detection systems with PAM data in hyperbiodiverse regions such as rainforests \citep{browning2017passive}. This is a bias that also extends to availability of suitable training datasets, and these combined have been described as being a major gap in the field at present \citep{browning2017passive}.

This lack of research effort is due to key difficulties of automated detection and classification in these regions. A primary challenge is the generally increased levels of noise, obscuring signals of interest. Variability in background environmental noise level has also previously limited the effectiveness of fully automated systems \citep{heinicke2015assessing}, and rainforests are known to have both high variation and high general baseline levels of noise \citep{waser1977experimental}, Traditional machine learning techniques can be very affected by noisy weather conditions, with recordings containing wind and rain often having to be discarded \citep{stowell2018automatic}. The task of detecting signals of interest in highly biodiverse regions is made more difficult by the increased levels of similarity in sounds produced by different species in these more complex soundscapes \citep{zamora2016acoustic}. 

However, the emergence in recent years of ML methods that are much more resilient to noisy input data, and that are capable of learning optimal discriminative features automatically, has significantly increased the potential of effective monitoring in these regions \citep{browning2017passive}. An example of this is the field of deep learning. There is a clear trend in the literature demonstrating its capability to produce state-of-the-art results in general audio detection problems (\citep{joly2016lifeclef,knight2017recommendations,kahl2017large}, and it has been proposed to be a promising method in particularly noisy environments \citep{browning2017passive}. 

[Brief explanation of neural nets and hyperparameters?]

Further innovations have enabled these powerful techniques to be succesfully applied to small datasets, circumventing the previous limitation of deep learning approaches of requiring very large amounts of data in order to achieve good results \citep{kiskin2017mosquito,salamon2017deep}. One such innovation is data augmentation, the process of applying transformations to data to artificially generate new samples with which to supplement the training of deep learning detectors. This process has the added advantage of increasing the generalisability of automated systems, as augmentations can be applied that mimic a wide variety of conditions possible in real-world conditions (such as overlapping of the signal with a prominent noise) that may well be absent from the training data. 

These techniques can be coupled with data preprocessing steps to further increase the possible performance of automated detection systems in highly biodiverse regions, an example of which is the 'denoising' of audio samples. While it is a complex task to separate signal from noise \citep{ovaskainen2018animal}, various methods have been developed to to increase signal-to-noise ratios of samples, and this has been shown to boost the performance of automated systems \citep{stowell2016bird}. Overall, explorations of these emerging techniques are fundamental in leveraging machine learning techniques in highly biodiverse regions.

\subsection{Case study - deep learning and PAM to monitor spider monkeys}

As a case study, in this project I will attempt to develop a basic automated detection system for the endangered neotropical primate Geoffroy’s spider monkey, \textit{Ateles geoffroyi}, in which a trained deep learning model - a convolutional neural network (hereafter CNN) - will be applied to continuous rainforest audio recordings. This species is well-suited for this analysis as they are heavily reliant on acoustic communication, as a result of being almost entirely arboreal, frugivorous (with patchily-distributed food), and living in complex fission-fusion societies splitting into subgroups to forage \citep{ramos2008communication}. I will train this detector using prelimary data from a wider project for which this system is intended to be used as a surveying tool, to assess the current distribution of \textit{Ateles geoffroyi} over the large scale (X km!\textsuperscript{2}) Osa peninsula of Costa Rica in order to build wildlife corridors to connect currently isolated populations. 

This problem offers the opportunity to investigate how varying elements of CNN design and training affect their ability to act as generalisable monitoring tools in noisy rainforests. I will experiment with varying network architectures, and with audio preprocessing techniques (denoising and standardising). Due to the very small current size of the training dataset, I will also apply data augmentation, using several methods previously used on similarly small datasets. Finally I will experiment with a number of combinations of tunable elements of the CNNs - known as hyperparameters - to optimise the current system based on the findings of the most effective combination of preprocessing techniques. 

% As more data will be collected as part of the wider project, I will test the effect of increasing data on the performance of the system (termed a 'learning curve') to assess whether performance is limited by data availability or architecture design.
	
\section{Methods} 

\subsection{Data: collection and labelling}

The original data was continuous recordings of rainforest sounds on the Osa Peninsula, a portion of which was collected in December 2017, which was then supplemented by data I collected during one-month of fieldwork in May 2018. We used AudioMoth recording devices \citep{hill2018audiomoth}, which create minute-long files '.wav' files named with a hexadecimal code representing the time and date of recording, recorded at 48 kHz. We made the recordings by fastening the devices to trees for periods of approximately three days, orienting the omnidirectional microphone upwards and angled into unsheltered areas of the forest so as to give the best chance of recording clear spider monkey calls. The possibility of water damamge influenced how they were placed (for example slightly sheltered by vegetation); however, some water damamge did cause some data loss. Nonetheless, over the two recording periods (plus a further one since) we collected a total of approximately 2000 hours of data.

To create the dataset of 'positive training examples (clips containing a spider monkey whinny), a primatologist with four years of experience listening to spider monkey calls listened to 191 hours of the recordings, seperating out minute-long clips containing the signal of interest. She then created label files in the software Praat \citep{praat} containing the start and end times of periods with and without the call. Using custom functions written in Python, I clipped the audio files into three second 'positive' sections containing a call. As calls were approximately 1 second long on average, with standard deviation of 0.3 seconds and the longest recorded being 2.1 seconds, I decided that a three second window was suitable. \cite{crump2017designing} reported that it was most beneficial to train their CNN detector using positives from as many different locations within the study region as possible, suggesting this was due to increasing the number of unique individuals recorded. Due to a lot of the data labelling being done before the start of the project, most of the positives (67/124, 54\%) used to train the network were from only one location. However, a portion of positives added in the later stages were from different locations (31\% and 6\% from two sites recorded during the data collection period, and a further 11 calls, 9\%, recorded in the same region but taken from CORNELL WEBSITE with permission of X). As this detector is intended to only be used in one region, I only used training from individuals in that region as recommended by \cite{knight2017recommendations} (forgoing the opportunity to add additional positive clips from \textit{Ateles geoffroyi} available).

I created the 'negative' training examples (three second clips known to not contain the signal of interest) in a three ways: (1) random sampling in call-containing minute-long clips from regions known to not contain calls; (2) carrying out a process of 'hard-negative mining' (as done by \cite{mac2018bat}) in which an early-stage trained version of the detector was ran on minute-long clips that have labelled call and non-call regions, separating any three-second sections classified as being postive (but known to be negative), and; (3) early-stage versions of the detector were also run on entire folders (one recording location, over a period of days, constituted one folder of files), and the same individual that originally labelled the calls listened to a large number of the positively-classified clips, separating out any false positives (clips not contain the signal of interest). I applied the hard-negative mining technique as \cite{mac2018bat} reported significant improvements as a result of this training on more challenging examples. 

In total, the original dataset with which to train the network consisted of only 124 positive clips and an equal number of negative examples (classes balanced as done by \cite{mac2018bat} and \citep{kiskin2017mosquito} for similar neural network binary detection problems). Where possible, for a given location I balanced the number of negatives with the number of positives so as to not introduce any biases by overrepresenting certain locations (which may have had different levels/combinations of background noise). This was not possible for the 11 calls donated rather than recorded by us, and so I balanced these few with further negatives from one of our recording sites. For locations with both hard-negative mined negatives and randomly sampled negatives, I added an equal ratio of both.   

\subsection{Data: preprocessing, augmentation}

I converted all raw audio clips to spectrograms using a fast Fourier transform - a mathematical process which decomposes the audio signal into the seperate frequencies that combined to form the signal. I used the Python package Librosa (CITE LIBROSA) for this, with parameters X X X. Specifically, I created mel-frequency spectrograms, in which the frequency bins are scaled logarithmically, thereby placing lesser importance on distinguishing between higher frequencies. This stage, mimicking how human ears process sounds of differing frequencies, has been shown to be a succesful transformation for data reductionality (reducing training time of neural nets), and is commonly used in state-of-the-art deep learning audio detection systems \citep{stowell2018automatic}.  

To denoise the spectrograms, I chose the denoising function of \cite{aide2013real} - also used in the competition-winning binary detection system of \cite{kahl2017large}. This function works by subtracting the mean amplitude of each frequency bin from all values in that bin, keeping only particularly loud signals present in the spectrogram. 

% To standardise the inputs (bounding them between 0 and 1), I divided all values (amplitudes) in each input spectrogram by the largest value in the spectrogram (pers. comm  X Sethi).

I implemented several data augmentation methods used by a number of teams working on similar problems e.g. \cite{kahl2017large,sprengel2016audio}. These augmentations were (1) adding a varying amount slight distortion (Gaussian noise) to the mel-spectrograms once generated, and (2) blending signal-containing and non-signal-containing files to produce spectrograms  To do the latter, I selected a number of three-second 'noise' clips, and the augmentation function would randomly select from these, add together mel-frequency spectrograms of the 'signal' and 'noise' clips, and renormalise to ensure the background noise levels had not been articially doubled. To implement these stages, as a spectrogram is represented as a matrix of frequency rows and time-step columns - with the values representing the amplitude - it is a matter of random additions of small values or combining two spectrograms representing seperate noises. While the data already suffered from a problem of high signal-to-noise ratio, I implemented these stages to increase the ability of the network to determine that signals partly masked by noise should still be detected as being true signals.   

I also developed a random crop augmentation function, in which the positive signal is repositioned within the three-second sample with a high probability that it is at least part-way cut off (retaining a minimum of 20\% of the call within the window). This was to ensure the that network was trained on calls that had been interupted part-way through, a stage I belied to be important as the full system splits minute-long files into three-second clips for testing, increasing the possibility that any calls present will span seperate input clips. 

To determine if any of the manipulations (preprocessing/augmenting) had an affect on the performance of the system, for each metric I compared the mean of the ten best-performance values with the corresponding mean metric value of the control (the CNN trained using unaltered data). As each of the ten training runs per manipulation did not consist of independent samples (each sample was used as training data in nine of the models), I used Wilcoxon signed ranks tests for the performance comparisons, as they have more statistical power when the assumption of independence is violated, and do not assume normality of input data \citep{brownlee2018statistical}. 

\subsection{Detector: design, optimisation and training}

As it is very challenging to entirely build an effective CNN, I followed the general practise of using the general architecture of state-of-the-art network designed for use in a similar detection problem (sound classification using small datasets with data augmentation) \citep{salamon2017deep}. The network hyperparameters that commonly vary between papers tackling similar detection problems, and that are known to influence model performance and generalisability, include the optimiser chosen, the learning rate, batch size, the type of activation layer, inclusion or exclusion of batch-normalisation layers, the dropout percentage, and number of neurons in the fully-connected layers at the end of the network (see GLOSSARY for a full explanation of these terms). To test all possible combinations of the hyperparameters under investigation I implemented an approach called 'grid-searching', which trains CNNs on all possible combinations of the choices for the hyperparameters to determine the optimal combination on a problem-specific basis. The type of data (preprocessed/augmented) I used for this investigation was that which resulted in the best performance of the network prior to optimisation. 

I trained the neural net for 40 epochs (a measure of machine learning training time, where one epoch is the number of training steps required for the network to have trained on every sample in the training set). This was because preliminary investigations showed that this was enough time to record the optimal perfomance of the models before overfitting occured - a common issue with small datasets in which the network begins to learn the exact patterns of the data rather than the general trends, reducing its ability to generalise to unseen data.  

\subsection{Detector: evaluation}

Machine learning classification algorithms can be evaluated using different metrics, which assess performance taking into account different combinations of the four classification possibilities: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These are obtained by training the algorithm on a percentage of the overall dataset, with general practice being to use 80\%-90\% depending on total dataset size, and then using the model to make predictions for each of the samples in the remainder of the dataset (termed the validation set). In choosing which metrics to report when comparing the models trained under  different conditions, I followed the best-practise recommendations of \cite{knight2017recommendations}. These are to report 'recall' (number of calls detected, TP, as a proportion of total calls present in the validation dataset, TP + FN), 'precision' (ratio of calls detected, TP, to total clips classified as being positives, TP + FP), and 'F1 score' (the harmonic mean of recall and precision). A high recall value would represent a situation in which a lot of calls in the validation dataset were correctly detected; however, as a detector that learns to classify every sample as being positive would give a perfect recall score, combining recall score with precision score (which penalises for number of false positives) means that the F1 score is a useful way to summarise overall performance of the detector. I also included the metric 'accuracy' (total number classified correctly, positive and negative, as a proportion of the total), to enable comparison with output of grid-search optimisation as this was the value that was able to be chosen to be maximised. 

For each preprocessing manipulation under investigation, I used stratified 10-fold cross-validation as it is regarded to be the most comprehensive method of evaluating the performance of a neural net. In this method the complete dataset is split into 10 'folds' (maintaining the proportion of positives and negatives in each fold). Ten CNNs are then created, with each being trained on a different withheld portion of the overall dataset, allowing average metric values with an indication of measure of spread to be obtained. As the metric value for the validation dataset could possibly increase but then decrease over training time if overfitting occured, the metric value I took was the maximum recorded for the training run of each model. This is equivalent to the common ML practise of assessing the performance of a trained model by reporting the highest metric value observed over training time (e.g. as used by \citep{norouzzadeh2018automatically}). 

To evaluate the effects of the data augmentation methods, I created functions to seperate out training and validation sets prior to then augmentating each individually. This was to ensure the complete independence of the training and validation sets, a crucial step which can otherwise inflate evaluater performance. Due to time constraints, this did not apply the same rigour as 10-fold cross-validation, but the train and validation sets were randomly generated for each of the ten training runs. 

\subsection{Overall system design and functionality}

The overall detection system iterates over folder of one-minute long files (as produced by the AudioMoth recording devices), processing each for the presence of the signal of interest. For each file, it splits it into twenty three-second clips, which are sequentially inputted into the trained CNN. Each resulting activation value of the last (output) layer of the network (a value between 0 and 1) is checked, and if this value is above a threshold activation value (e.g. 0.5) - specified by the user at runtime - the clip is considered to contain the signal. The metrics reported for the CNN peformance were tested with a threshold of 0.5, but I have included the option to alter the threshold when running the system to allow for the altering of the sensitivity of the detector - increasing the threshold would decrease the number of false positives, but also increase the number of false negatives (calls that are more difficult to detect). When running the system, the user can select which of the preprocessing techniques (denoising, standardising, or the combination of both) they would like to apply to the inputted data.

I programmed the system to give two outputs. The first is a folder containing all detected-positive three second clips (informatively labelled with the original file name of the 60-second clip, the time location within the clip they came from i.e. the start and end of three-second interval, a number representing which of the total number of detected clips from their file they were, and the activation value multiplied by 100 acting as a proxy of confidence. The second output is a summary CSV file of all detected clips, with containing file name, approximate position of detected clip in file (secs), the time and date of recording of the original file, and the confidence of the CNN's classification (again, calculated using activation value of output layer for each clip). 

\section{Results}

Of the different preprocessing techniques I investigated - standardising, denoising, and the combination of standardising and denoising - none increased the performance of the system for the metrics recorded (all p-values greater than 0.05). Training CNNs with all augmentations implemented (crop augmentation, Gaussian noise augmentation, and noise sample-combined augmentations) negatively affected the performance of the network as reported by precision, recall, and F1 scores (all at p < 0.0001, W = 4, 3 and 0 respectively), whereas the metric accuracy was not affected (p = 0.38, W = 38).   

The learning curve investigating classifier performance at progressively increasing training dataset sizes (see figure X) showed that, for the very limited sample sizes at present, an increase of number of positives (with a corresponding increase in negatives) did not increase the performance of the classifier.

Some patterns emerged as a result of the grid search investigating optimum configurations of hyperparameters (see TABLE X). The best activation layers to use were X, it was better to X include/exlude batch normalisation layers, and the 64 units in the fully connected layer originally in the network of \citep{salamon2017deep} were better/worse X than increasing this to 128 units. The suggested values of dropout were p = X in the first dropout layer and p = X in the subsequent dropout layer.

\begin{table}[]
\begin{tabular}{l}
Model with grid-search parameters (optimum combination in bold) \\
Conv1, 24x5x5, Stride (1,1)                                     \\
Max Pooling, Size (4x2)                                         \\
Activation: relu/elu                                            \\
Batch Normalisation: with/without                               \\
Conv2, 48x5x5, Stride (1,1), Valid Padding                      \\
Max Pooling, Size (4x2)                                         \\
Activation: relu/elu                                            \\
(Batch Normalisation: with/without)                             \\
Flatten                                                         \\
Dropout, p=                                                     \\
Dense, 64/128 neurons                                           \\
Activation: relu/elu                                            \\
Dropout, p=                                                     \\
(Batch Normalisation: with/without)                             \\
Sigmoid Output                                                 
\end{tabular}
\end{table}

When running the detector system as a whole, the time to process one minute-long file is approxately 1.553 seconds; therefore searching a folder of all files, collected over a recording period of three days in one locations, takes slightly over one hour.  

\section{Discussion}

Fundamentally very challenging 
So many different things that can be altered. Hyperparameters, data input type, network architecture, and getting excellent performance from a network can be very difficult

Findings: all_augmentations seemed to have a slightly positive effect on recall.
However, none of the factors investigated had much of an effect.

\citep{stowell2018automatic} reported difficulties in their deep learning system detecting signals that were faint, with interference from masking noise being their second concern
- for mine, highly likely monkeys will call multiple times (pers comm. Jenna) 

Key element was that more work needs to be done to seperate the signal component from the noise component
I certainly believe more sophisticated noise reduction methods should be explored, such as done by Versteegh et al. 

Due to time constraints involved in designing the general system as a whole plus experimenting with preprocessing and augmentation techniques, I elected not to explore transfer learning, another commonly done method for applying deep learning approaches to small datasets
 
However, the results of these factors show that, while slight improvements in performance were recorded when applying all augmentations in combination, the performance of the neural network was very poor, and therefore I would strongly recommend that further work investigate the potential of incorporating transfer learning (e.g. see \cite{strout2017anuran} for a very similar application of this approach).

I would strongly recommend that future work reduces the dimensions of input by only taking known frequency range of spider monkey calls, such as was done by \cite{mac2018bat} in a similar detection system, as this would lessen the chance the system may learn the pattern.

Increase in metric of training dataset shows that the network is training, but clearly only to some pattern that isn't the signal.
High values 

While I changed investigated optimised only the hyperparameters, the poor performance suggests that it may be worth exploring other general architectures

Apply 'pitch-roll' augmentation of Kahl et al. I was unable to do so within the time but this is an augmentation that showed big increase in classifier performance 

Could try training with a biased ratio. as there is so much variation in the negative data, this may increase performance  

CONCLUSION:

Going forward, functions can be used in continuing development of system
More data will be collected, I will show Jenna how to add this data and train
Once threshold of 500 calls is reached, architecture effectiveness will be reassessed 
Deep noising method tested did not have an affect with such small sample sizes
Potential: citizen science, big labelled audio datasets
Plus, big labelled camera trap datasets are offering similar potential for large scale monitoring techniques, sharing many of the benefits of deep learning combined with PAM 

I trained my networks for a length of time in which I should have seen some overfitting, and so classifier performance would fall. However, can use early stopping, which only saves weights if they led to an increase in performance. 

\section{Glossary}
CONV LAYERS - a small  filter, or 'kernel', is run over the image, capable of detecting general elements such as edges in images in early layers in the networks. 
POOLING LAYER - a strength of CNNs, pooling reduces the overall dimension of the input data but retains the most important information. It gives the network the ability to not need patterns to be of exact shapes, it will still be able to recognise patterns if structure is generally similar (called translational invariance). A small window of specified dimensions is slid sequentially over the input layer, with only one value (often the largest, in so-called max pooling) in the region of the window going forward to the next layer. 
backpropagation - the progressive altering the weights (strengths) of connections in the network to lessen how incorrect the predictions of the network are
optimiser - the mathematical method determining how the network should carry out 
batch size - how many samples are shown to the network before an instance of backpropagation occurs 
activation layers - a step determining, based on the input to each neuron in the neural network layer, whether those neurons should in turn fire
dropout - 

- generally, neural nets recognise patterns in data, through a process of learning which elements of inputs to focus on
- trained on 'labelled' positive and negative clips 
- built of combination of layers with different functions, but typically consist of:
- convolutional layers - a small filter, or 'kernel', is sequentially slid over an input matrix (e.g. a visual representation of a sound), detecting patterns in the data. In the early stages of a CNN, this will likely be looking for very simple elements, such as straight lines or curves; however, convolutional layers deeper in networks are known to be capable of detecting very abstract patterns. 
- pooling layers - 

 pooling layers, activation layers, fully connected layers), can be stacked in many combinations
- layers are made up of nodes (neurons), with connections (weights) to nodes in layers before and after
- output of one layer is input to the next 
- if trained on sufficient amounts of data, first layers act to recognise general features such as straight lines and curves, with deeper layers capable of recognising more abstract patterns
- initially, the predictions of a network will be random, but through a process of penalising the network for incorrect predictions (by checking prediction against label), a mathematical process called backpropagation alters the weights connecting neurons of separate layers so as to make the network more likely to make the correct prediction in future  
- the networks 
- over time, with sufficient positives and negatives, and an overall design that can permit the network to learn 

priority:

get results in 

figure 1: architecture
glossary in middle of methods
figure 2: manipulations on original dataset
figure 3: small sample training run with a spike
figure 4: large sample training run 
figure 5: manipulations on augmented datasets 

Discussion

Interpretation of results
- fundamentally, the network trains (see increasing value in FIGURE X BIG DATASET LESS SPIKEY), with performance metric on the training dataset increasing over time. 
- however, it does seem to be overfitting (see FIGURE BIG DATA LOSS), with the rise in performance on the validation dataset coming to an end  

denoising:
- on both the small dataset and large datasets, the denoising function caused a drop in detector performance 

standardising:
- on small dataset,
- on large dataset,

augmentations:
- more aug better than less, but tested on specifically auged data

Limitations section:

- method of 'take max value', equivalent to only taking best model as that which scored highest, is problematic when evaluating performance of models on small datasets for my data
- some samples may be easier to classify than others 
- as weights are updated when the network is trained on number of values in batch size, it is possible that, for a smaller dataset, there is a higher likelihood that a batch may be made up of these more easily classifiable samples 
- this may explain the occasional spikes in metric values which are not reflective of average performance. Therefore, while it may be inferred that there seems to have been no significant difference in classifier performance for any of the preprocessing techniques I tested, definite care should be taken when interpreting the absolute performance values shown   



Further work
- more sophisticated denoising functions, placing heightened importance on increasing signal-to-noise ratio
- transfer learning
- 

Conclusion



I changed my test set each time. Did that because REF said

add code publically available at github.com/dgabutler/spider-monkey-detector

\bibliographystyle{agsm}
\bibliography{thesis}

\end{document})
